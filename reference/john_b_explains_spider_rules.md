# Spider Rules Explained

## Core Concepts
- Spiders follow links within allowed domains only
- Must have a start URI
- Can control scope via Include/Exclude patterns

## Key Controls
1. **Start URIs**
   - Required starting point
   - Can be inferred from project

2. **URI Patterns**
   - Exclude URIs: Block patterns
   - Include URIs: Override exclusions
   - Example: Block all except `/product/*`

3. **URL Processing**
   - Truncates changing parameters
   - Handles hash fragments
   - Uses canonical URLs

## Common Configurations
- Single page: Start URI + no follow
- Product catalog: `/product` + include pattern
- Full site: Start at root + domain limits

## Integration
- Rules affect both crawl and live behavior
- Moxie respects these settings
- Maintains consistency across environments

We are talking about spider rules

John B. "don't follow anything"

John P: "is there any way to say I want to do slash product"

John B: "yup"

John B: "Scroll down"

John B: "Include uris is an override to"


John P: "they have a core domain"

Dan: "before we all get crazy"

Dan: "we currently have the behavior"

Dan "NOw we already committed"

JJ: "within these walls"

John B: "sitemap is another"

John B: "truncate urls as a hook"

JJ: "the canonical url"

John P: "is that the type of thing"

John B: "we always want to truncate"

Dan: "it would check you out"

Dan: "If you added a user story"

John B: "you took that same picture"

John P: "weglot has a simple api for that"

John B: "in the future"

John B: "scan the site"

John B: "the medlib"

- John is still going through the spider rules

John B: "why"

John B: "we want to throttle"

John P: "some of these things"

Elad: "we could also give them a range"

John B: the schema has a range

John B: "a lot came from microsoft"


John B: "or you can't get in"

Dan: "it is actually way more than that"

Dan: "you want to set any number of headers"

- JJ said somethinng about AI

John B: "query selector clicks"


John B: "you can tweak these settings per spider"


-we are moving on to talkin about the translation profile as a whole, not just the spider as a whole


John B: "block patterns"

What does it mean to get multiple? What is one slash product? Yeah, yeah. So it will start the slash product and will go... It will spider so crawl the website. It will capture that page. Any links on that page are going to follow those. Even if they're not slash products? Yes. As long as the domain is one of the allowed domains for this product. Like cacme.com or spider in their site, it's not going to go to facebook.com. Follow up any links that are not allowed. But yes, it will walk anywhere and every link that it finds. If you did... What if you did slash product as your start URI? And you had follow links turn off. It would go to start URIs and it would do slash products. That's it. It would just do slash products. I want to spider this page alone. Some customers do that. They start URIs, they do 10 things. That's it. Follow links no. It's better to take this one. Yeah. Right at these 10 pages. And there's no way to say... No follow links. Slash... I want to... Is there any way to say I want to use slash product and everything underneath it? Slash product slash fall 2024 catalog. Slash product slash... Which you would like to slash product slash formats. Start URIs is slash product. Yep. And then the... Just go down a little bit. Clue URIs and exclude URIs. Exclude URIs gives a pattern to say don't follow any links that match this. So in there you say it's a regular expression to the dot star. Which means don't follow anything. Include URIs is an override to exclude. Include URIs says accept for links. And then it was pattern slash product again. So these should... So then URIs should actually be up with that. And to start URI you can make... Do you need start URIs? You need to go start somewhere. It's right over here. Yeah, I have a start. And then my schema. You got to start somewhere. Yeah. Is that something that the user needs to input? We can infer it off the right... We always guess it. It just depends how you're starting. You told us you want to do... It's like a hidden setting. Right, because if we're doing the third one, we're starting with a project. That project has a... Especially a GLB... Dan, feel free to chime in here. But especially a GLB project, the way it works, because it is... They have a core domain. Or a primary domain. And if there are other domains, we've set them up to be staging or whatever. Okay, so we're curious. You might want to tell this fire to the staging place. Not that. We're going to all crazy. Excuse me. Well, hey. My objective is to try to make the config something that the user can look at. And be like, "Oh, yeah, duh. I want to spider my product catalog. And it's obvious to me how I do that." Okay, but I think we're already... We're supposed to fill out a intake form of our main project. This car was test-rided. Have we agreed that JJ is going to use as the config for his spider this config? Yes. Okay, you didn't that first. No, I'm going to now. As of yesterday, I am all over this. I'm taking notes. There's AI listening. I'm about to show you... If you want, I can show you how to edit this config right now in the ways you guys are talking about it. You guys can give me a description and I will edit it. Yeah, you'll see all the good things. Originally, we were going to... This double-JS project was going to come here and we supported it. The only way to get all the good parts of the spider using this tool is using the same thing. So, we could... We have an architectural decision mode right now. We currently have the behavior of the translated site controlled in the same place as the scope of a spider. When in reality, those are all the same. We're already committed to make a new table called Zappy's Spider Profiles. We could decide to adopt a different profile just for spider that makes more sense. Now, the exclusive URIs, including URIs, the Trunkator, OZ, the girlfriend, all those spider types are handed to Moxie on the live sites. Yeah, this is perfect. Moxie obeys those spider moves. Okay. They're like, "Nive of them. They must." You could... If you could find someone else, at least attempt a match. This is one of those things. Yeah, we need a match. All right. That's one of those things where we just want to pick up the momentum of all the great work you guys have done over the years on this and stuff and just build on top of it and not recreate that. There's some things that are immutable. There are some things that are immutable and that are valuable here, that are very valuable here. Yeah, very. Okay, there's a lot of engineering that's gone into this and lots of bugs and lots of tweaks and it's a real golden nugget source of truth from my perspective when I watch, like all the thought that's gone into all the tweaks of the settings and what they mean. Maybe they're wrong, but to me it seems like this is pretty much something that should never be violated within these walls. So, you know, you can't spider and also mask much what Moxie does in real time, right? You can't spider and give them a list of segments and then when Moxie actually runs it gets something else. So, it has much. Let's continue with the first one. Yep, okay. So, let's restart. This site map is another way for the stick user to specify an alternative site map of food you've got, X and L. Here's a site map. It's not the standard site map. So, we don't need to expose that here. Well, if someone in the movie wants to put a site map in their crawler, it's easy to expose. Yeah. I don't know if that was a huge case. I'm going to keep the UI as simple as Moxie can. It's easy to expose. It's just a site map. I mean, the only thing I would think about is if very recoverable happens, a client will say, "My scope is a site map." It's very rare. What's more common is, until they separate, can you mic into the spider form? It would be, "Here's my site map. Can you check it for errors?" And I created a broken site map. That's really a different tool. It's a different type of spider. I would say we can leave that. It's there, but you felt it was like... I can demonstrate. You guys want to demonstrate? I'll demonstrate all the stuff we're talking about to you right now, if you want, on an app so you guys can see it. Because the one thing that I think that's important is that it is coherent. Yeah, you might as well just finish running through just different parameters. Yeah. I'll go for it. I'm taking notes. I'm taking notes. It says, "Internate early with that and the hook drops off all the road approaches." I'm taking notes. "Internate early with that." I can't understand the session I'm using. Yeah. You know websites that have so much constantly changing the URL parameters that every time you go to the page, it's completely different URL. We don't want that because it's the same damn page. We want to use app use everything we know about it as this page. Yeah. The canonical URL. The canonical URL for it. So that's the sort of section like beyond by default? Yes. Yeah. Trunk8Girls acts. No, it's very different per websites. And girl parameters are so unique. You don't want to truncate and hook it this way out. It's very important. There's a lot of things. It's unique, very custom. So I think it's just a list of places you should truncate and rely on to treat girls and say, truncate and rely on ashes very much similar. I do some of it already. A lot of truncation because a lot of stuff in the database has to line up exactly. And you definitely have to always concoct things in order to-- but not as much as a spider. I haven't taken a complete look. But even simple spidering will expose you to this problem to some extent. It's important. The list of URLs belonging to the project will depend on how you truncate or accept it. Truncate or hash, the hashtag is-- follow us, it's true. We will always truncate or like hash. The hashtag is a reference to an item on the page. It's not to do with it. It's a locator. It's a local. I always wanted to show you that. There are some stupid customers who use the hashtag in a pack. As a pack. You look at the browser and you look at what the browser considers the URL path and you're hashing. It's not what they think it is. It's screwed up. But unless you turn this off and say you truncate or hash false, their paths are all gone. So this has to be done for some customers. But if I develop this all, this truncate will have an up-and-down-to-the-past sign. And we move our parameter, the next one is very, very similar to the truncator that, except truncating from this point on, it picks up a particular parameter. Just a group session, I think that one. It's a specific URL parameter. So those three kind of control the unique of finding URLs that we find. Both Moxie and Zappy's understand the hash of the URL. So we go to translated page. We ask for pre-translated, we ask for trans-transmitter and for information based on the URL hash. Give me all the blocks that you know about for this page. And that page is the URL hash that's known from the construction of the URL, which depends on these parameters. So this is very, very specific. Control that. So the other way, not to get into it, just thinking about it, I want to stay on this. We could show them a URL either from their site or a sort of generic one and have these different things highlighted in a way that visualizes what you're doing. Like when you say truncate app and you see question mark or you see that, you know, I want to actually, they can just drag it off. You see, I'm just thinking about it. But, all right. Yeah, you could present that to me. We can even talk about AI, right? You've noticed that all of their URLs have something like a session ID or something that's there. I may want to suggest this appears to be something unique that you may want to use. Yeah, yeah. Use site maps is saying to use the standard site map that a website map is not. All that does is basically add to the starting arise. Go to the site map, give me all the URLs that they're suggesting I go to. Websites have suggestions for robots to say go here. So we use site maps. But if I will use it, but you can turn it off and say don't use it. Yeah, I don't know. Yeah, I think it's easy to expose. Using my standard site map to find things. Robots through instructions. Websites have instructions either in their site map or in their inside of the data in the website to say don't go here, don't touch this, don't look at this. And despite it, we have a little way that if the robot will say don't go into the section of the website, so don't go there. Why would you turn that off? Because they may, for translation purposes, they do want to go in there. That our robot will find its purpose. All the links we covered, what does find JFS? That one, I don't know. Find JFS links was on a website. There's tons of content that can be hidden behind actions. A lot of times you expand the menu and it makes it visible and that's stuff you can see, but there's a click on a button that does an Ajax call "FetchesD" and fills a table. But what I will run in is an app. We auto-hit it again. It says if it's always turned on. What is always turned on? Well, when you follow the link on the page, there's no links. It's a website has a button, a button click, fills in more data, fills the whole folder, the middle section. Find JFS links. Or it can navigate somewhere with a button. It goes around and clicks on anything that's clickable. And then sees if the place you ended up on is the same place that you are. It creates an iPhone and goes to... We should have called that "ClickEverything" Exactly, we should have clicked everything. It is specialized live in the market. And quite often, a lot of pages have problems with that. That's how we found all this money product. It's off by default. That's how we got all the PII. It's turned off by default because a lot of pages, a lot of websites have problems with that. You're trying to open up your things, you're trying to go and find stuff that... There are some sites, we created this work because there's some sites that have content behind these things that's not there. So let's click every button. Let's click every button. I don't even think we should put that in. MVP one. This reminds me of something. This reminds me of a Selenium script that we had written for L'Oreal. And it would basically test a checkout. So it would check you out to make sure the checkout worked. And we had a discount, which would just void the order. This ran once a week. Then we had a tester add like 10 thousand dollars to L'Oreal products, which is probably two twos of toothpaste. And L'Oreal had broken the discount link. This was a live-sum test. And we got a call from L'Oreal Finance. It's fraud team. Why did your account post a 10 thousand dollar sale? We don't want to honor this. It doesn't look right. Does code don't charge us TDZ number anymore? No, that's not a code anymore. It's like, hey. So this is my story. Yeah, but I can imagine. So I'm going to log in. And then log in to like Chase Meg account. And it clicks on every button. I can see your bad. So transfer my balance to Dan on our left. So I don't think we should expose that. No. If we ever need it, I think it should be done a different way. This was like the Selenium approach. The actual text is created for men lit. Men lit wanted to spider on their sites and capture PDF and word docs. So they don't care about websites and care about capturing documents. And then we-- so we wanted to not capture text for someone who doesn't even care about it. So we had to be able to turn off text to capture it. And then download assets is the-- And then we had to pull them as well to get down on the PDF and word docs and such. It doesn't make any sense for you. Well, we might not want to handle it for MVP, but there is something that people do as part of getting a quote. They'll say, hey, you have 15 PDFs on your site. You want us to chart and localize those for you too. It's a common service, but often we use our spider to surface that content and then try to bundle it with document translation service. Which one is that? That would be the download assets. And possibly download assets plus capture text. Capture text. Yeah. So we added a spider user story jump that was scanned for assets. The profile would be capture text falls, download assets, true. So we're going to have to add a production anyway. Yeah, I think it can be down the road. I think every other-- You included it in the first go. So that's a user story. Yeah. What happens? The user story comes. We don't still see images in the S3. Why does it not like that? It costs us money. I thought that was the answer. I was just making sure. If it has aggregate fist in the sentence, you can hear me. It's about heart rate. Remember the time we OCR all was in the S3? That was bad. That was like-- We were testing. We weren't testing OCR. Yeah, we did do $10,000 from OCR. We turned that up quickly after that. If you set meta robots, I missed it. No, it does. Check meta robots is very similar to use robots. This book will be full. Robots, true. Every website has-- a lot of websites have-- every site map, they have robots to test on the main site about rules for bots. So read that and obey it. But if you set that default, we won't bother doing the robots. Check meta robots. In the meta tags, you can place throughout the HTML and the instructions of robots, follow faults. Then a robot will go there. So you can turn that up. But it's not the obey those rules. You can say that. Don't make it a false-- Like a fault, I think a LJS, you know, I think just turns-- Check meta robots, false robots, false-- I don't know why we just kind of turned it off. Native images, we're just talking about capturing out images, I think. Yeah. Okay, tags gives image backgrounds. The splatter captures not only the fact that they exist, it downloads them, stores them in S3 bucket. It gets a hash from the S3 bucket, and it tells Zappies, it fills in a database table, with the S3 URL, the actual original URL. It also has information about the size, width, height, a bunch of information, number of lengths, all that, about the actual image. It's useless because we don't translate images about the brand. That was the intent. Ultimately, we're going to have it up in S3, it's a bucket with the T key of languages that have translated images, and Moxie has the capability to actually switch-- It actually has the capability to translate an image. It'll replace this image with that image if that all existed, but it doesn't exist. We don't have that. We probably shouldn't even have this in the LJS. What is it? There's no reason to actually store the image. If the image is a hat, then can't you just have the user using a UI that you don't have yet upload an image? You think they'd do it right? Right, you only need the URL. Yeah, you're right. So you say-- The hash is a hash of the light content of the image. So if I took a picture of all of this in here and posted it to my website, and we had John's meeting, you took that same picture shared and called it "Din and John's meetings." Different URL, different name. But when the spider finds those two, the hash is the same image. So we don't duplicate images as three. Alright, we got some really simple UI files. It's just replace this URL with this URL in front of you. We had that in the actually-- Target configurations, we can replace images. We can do that, which is what anyone who doesn't uses that. So we don't leave this in the spider at all. Same thing with download assets. We certainly don't need to download. Download assets would be in the future. I think Joe would danger set a short while ago. It kind of goes along with after-tests faults. I want to-- It may not sort of download as in some-- In our case, you want to scan the site for PDFs. Yeah. And by the way, you can use all your PDFs to do a transaction. Yeah. We're not actually downloading. Well, tarantula is downloading them, putting them in S3. And then the med-lipped team would then get a zip file weekly of that. And then they'd go take it away and remove it from you. But that doesn't-- We're doing that anymore. We would want to download, but we may want to search for it. Maybe? Yeah. Crawl the way. Crawl the way is used a lot. By default, we have an adaptive crawl the way. We kind of measure response time, and it kind of throttles itself. I think JJ's already kind of implemented some sort of a delay in crawl we have to. I think our websites are going to probably think we're a span attack if we try to go after it. Yeah, well, they figure out pretty quickly, at least with the JJ's old spider site. They're like, "Hold on. Now this guy can click fast." Or reasonably fast. I don't know if we want to-- I guess we would build it into-- No, you would crawl the way. You definitely would. What is 10-ounce milliseconds? It's milliseconds. So 10 seconds. That's a random example. We should crawl the way in the higher milliseconds or the lower ones. 1,000 milliseconds. Alright, max pending. That's a weird one. But, well, max pending is the number of concurrent width URLs to go to at any time. It's the number of threads. You do five pages at a time. You want to do 10 pages at a time and make max pending content. And do 10 at a time. If you didn't-- What's the plot? Because if we hit-- Because if we have from the site map and the starting RIs, there's 15,000 more URLs we have to go to. 15,000 threads will hit 15,000 web pages all at the same time. And capture all the data and it will be done and all finished. And then we'll have more-- the website will block us with other resources. We want to throttle ourselves going too late. Some of these things are things that we-- Some of these things are things that we need the user to tell us. Some of them are things that we need to do for them. Right? Yep, it all depends on context. And that's the beauty. We could easily say 10. And don't even show them-- You can't even try it in one attempt. We're not going to let you in the car. You can do less, you can do more. And then you don't even show them. Yeah, okay. I'm going to expose it saying, make this-- how fast do you want this crawler to go in the swire? And just brings that number of-- that number up higher and lower. More threads and less threads. So we could expose it as speed control. So it's all about speed control and then throttling. Maybe I'll just give them a range. Oh yeah. Yeah, the three, like 15,000. The schema. Yeah, it's like this will go faster, but you'll have a higher chance of it failing. Because you got the walker, what are-- this one's slower. But it's probably the same go up. The schema defined for this has a range. It has a minimum actually. So you can't go-- you have to go one to one. You can't do more than that. You can't do more than that. All right. Recursion. Recursion came-- a lot of these controls actually came from Microsoft's documentation where there's either API and such. A lot of sites will repeat the URL. Because it'll be like a loop that just keep adding to the UI. So you're like, slash main slash foo. And then there's a link and it says foo. I'm going to say foo. I'm going to go foo. And this is a control to save. You see as foo, I pass to those and I go four times. Don't-- okay, you're not-- don't follow that. I don't think we ever really, really get this use case situation. But it's a safety net. All right. Yeah, let's just blast through these nice things. Headers are for some websites that must-- no, must get set in-- or you can't get in. Must get what? You have to set a header called foo.var. Or you can't get into the site. It's kind of like a login. But it's a-- it's a-- it's a header. Well, we-- it's actually way more than that. We sometimes want to set it at user-agent. I'm sure you can use it in that way. No, we allow-- it's a vector, right? Yeah. It's a-- Well, is it an object of objects? It's an object of objects. Okay. So there's a key value, key value, key value, key value. Yeah, so-- To have-- To concern UI, John P, you want to be able to set any number of headers, just like we do in post-man, for example. You're getting-- you know, if you want to do presets, we can copy post-man's UI like if it's authentication header, we can fill out everything and they just paste it on the top of the point. You can go like this. They have to-- we want to go to base. Or if there's a user-agent header, we should give a picker of the usual user-agents when you're holding that button. You guys really need to see AI because why would you build UIs for this stuff when you can just paste this stuff into a text box and have them-- I mean, it's like-- we could spend a lot of time building-- I mean, for the top things, we need to build a UI, but most of this, you can just paste this stuff in. I'll do it for you. Do you want it to be like a map of that kind of spider? No, I have it. I'll show it to you right now. I can-- I'm not building it. It's like it's already built. Like, AI is already capable of all of that. Okay. Go ahead and take care of this part. This is all already working. You have to offer headers. Set these headers when you go inside. I am curious to see. I'll show it to you right now. I mean, this is all implemented. The next three logins, logins, pages, pages, yes, are a way of getting a specific site that has a login. Yeah. You can specify things to navigate Chrome, Chrome, driver, JS to anything, ch-ch-ch-ch-ch-ch-ch. You can go in and go in and go. It does work. It's really messy. We don't need it here. We don't need any of those. The query selected clicks was what I was kind of speaking of before where you specify a bunch of things, CSS selectives, to go click on to load new content on the page. Kind of like the next one. So cool. There's a button here or a thing there that you need to click on to actually load content. So it was a way of capturing more content from some sites or whatever you could do with a click. I don't think for anything key, I think we can not. Dynamic scroll is the same kind of thing. Princess Cruises, you go to the site and they know it's this. That's it. But if you scroll, more content. So this allows you to say scroll, scroll, scroll, scroll 50 times or-- OK, now it captures content. We probably should do it. We could probably do it out first. And walk delay is much like crawl delay. But it's not crawl delays per page. It's crawl delays from this page to the next page to the next page. The walk delay is you hit a page and you want to wait to find 100 milliseconds or 1,000 milliseconds. OK, now capture it. Because pages are slow and content's not there. You load, capture, you can go. You missed all the content. You've got to get these monolithic student pages sometimes. Can we upload default settings for this too? Yeah, most all of these have default settings. Just set them or not set them. The only default. We've done that on any x-ray. OK. Yeah, we are-- And then it's walk delay. And then exclude your eyes, include your eyes, you know what they are. Include exclude domains are really complicated. And I don't think we want to expose them. Include domains is a little-- I mean, I'm saying I could also include sales.foo.com as well as foo.com. But at sales.foo.com, I want backspin 87. I want headers to be this. And then change all of these configurations underneath that. It repeats all of that. So I can control per domain. Yeah, it's different than the other times. So we can have like a different screen. Yeah, exactly that. I think we ignore that. Yeah, I think we ignore it. OK, cool. Thank you. There is still a question of what of the translation in FAKE affects the spider? Every bit of it. OK. Now, again, some of those you don't need to expose. Some of you do. OK, no translate. Yep, I get that. Translate is an overrider in the translation. Yeah. Select your translate. Select your translate pieces of a page. More than anything else. I don't want to do this. So, I'm going to translate this for sewer elements and my frames. For hidden. For your doll. Slots. These are all very specific. Besides no translate, translate is a fairly common. A lot of these are very specific. No. I. More than anything else. OK. Some of these don't affect the spider. No translate will matter because it's selective in concepts. It's selective translate. Also, pseudo, eye frame, all of them are trans. I'm going to hit it to remove a few more tabs. Related to why I won't affect any of these, I might not affect the spider. I might select it more if I'm the spider. I'm going to select the other tabs. I'm going to select the other tabs. I'm going to select the other tabs. I'm going to select the other tabs. I'm going to select the other tabs. I'm going to select the other tabs. I'm going to select the other tabs. I'm going to select the other tabs. I'm going to select the other tabs. I'm going to select the other tabs. I'm going to select the other tabs. We don't care about the spider. We don't care about the spider. That second is tokenized numbers is important. The content doesn't matter because it's all a lot of translations. The problem doesn't matter. Tokenized patterns is very important. Tokenized is important because the attribute is no translations. Input value is not always a capturing container. No capturing. I'm going to select it with nothing. See, active element means nothing. It's just a translation. It's supposed to be a translation. It's a full type of pulse. No matter. As a whole, none of those are implemented by the spider. The spider just takes the hands of the monster. The monster means that the information is consistent with their life. If you're just doing a random sight, you can all the thoughts. You can just go using the thoughts. If you're using a profile, obviously the most common is to translate no translations. I guess you start to get into a lot of other pseudo elements. All in all, if you build a full configurator, most of these will be exposed. I'll show you a full configurator right now. You want to have a chicken cast?